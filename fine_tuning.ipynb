{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Roaming\\Python\\Python312\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\User\\AppData\\Roaming\\Python\\Python312\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "Loaded 1001 training examples.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 96/96 [08:51<00:00,  5.54s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 531.7817, 'train_samples_per_second': 5.647, 'train_steps_per_second': 0.181, 'train_loss': 0.24934768676757812, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine-tuned model saved to fine_tuned_semantic_search_model\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer, InputExample, losses\n",
    "from torch.utils.data import DataLoader\n",
    "import json\n",
    "\n",
    "# Step 1: Load the Preprocessed Dataset\n",
    "def load_data(json_file):\n",
    "    \"\"\"\n",
    "    Load the dataset from a JSON file and convert it to InputExample format.\n",
    "    \"\"\"\n",
    "    with open(json_file, \"r\", encoding=\"utf-8\") as file:\n",
    "        data = json.load(file)\n",
    "    \n",
    "    # Convert each entry to InputExample\n",
    "    examples = [\n",
    "        InputExample(\n",
    "            texts=[item[\"sentence1\"], item[\"sentence2\"]],\n",
    "            label=float(item[\"label\"])\n",
    "        )\n",
    "        for item in data\n",
    "    ]\n",
    "    return examples\n",
    "\n",
    "# Step 2: Fine-Tune the Model\n",
    "def fine_tune_model(train_examples, model_name, output_dir, batch_size=32, epochs=3):\n",
    "    \"\"\"\n",
    "    Fine-tune a pre-trained SentenceTransformer model using the provided examples.\n",
    "    \"\"\"\n",
    "    # Load a pre-trained SentenceTransformer model\n",
    "    model = SentenceTransformer(model_name)\n",
    "\n",
    "    # Create a DataLoader\n",
    "    train_dataloader = DataLoader(train_examples, shuffle=True, batch_size=batch_size)\n",
    "\n",
    "    # Define the loss function (CosineSimilarityLoss)\n",
    "    train_loss = losses.CosineSimilarityLoss(model)\n",
    "\n",
    "    # Fine-tune the model\n",
    "    model.fit(\n",
    "        train_objectives=[(train_dataloader, train_loss)],\n",
    "        epochs=epochs,\n",
    "        warmup_steps=100,\n",
    "        show_progress_bar=True\n",
    "    )\n",
    "\n",
    "    # Save the fine-tuned model\n",
    "    model.save(output_dir)\n",
    "    print(f\"Fine-tuned model saved to {output_dir}\")\n",
    "\n",
    "# Main Script\n",
    "if __name__ == \"__main__\":\n",
    "    # File paths\n",
    "    input_json_file = \"semantic_search_data.json\"  # Replace with your JSON file\n",
    "    model_name = \"sentence-transformers/all-MiniLM-L6-v2\"  # Base model\n",
    "    output_dir = \"fine_tuned_semantic_search_model\"  # Directory to save the model\n",
    "\n",
    "    # Parameters\n",
    "    batch_size = 32\n",
    "    epochs = 3\n",
    "\n",
    "    # Load data\n",
    "    train_examples = load_data(input_json_file)\n",
    "    print(f\"Loaded {len(train_examples)} training examples.\")\n",
    "\n",
    "    # Fine-tune the model\n",
    "    fine_tune_model(train_examples, model_name, output_dir, batch_size, epochs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Validating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded.\n",
      "Loaded 987 unique questions and 987 unique answers.\n",
      "Encoding questions and answers...\n",
      "\n",
      "Performing semantic search for validation...\n",
      "\n",
      "Question: តើមូលនិធិអភិវឌ្ឍន៍ព្រៃឈើជាតិមានប្រភពចំណូលមកពីអ្វីខ្លះ ?\n",
      "Top Answers:\n",
      "1. ក្រសួងការបរទេសនិងសហប្រតិបត្តិការអន្តរជាតិស្ថិតនៅក្នុងវិស័យរដ្ឋបាល ។ (Similarity: 0.7895)\n",
      "2. នៅជនបទកម្ពុជាប្រភេទឥន្ធនៈសំខាន់ជាងគេសម្រាប់ចម្អិនអាហារនោះអុស ។ (Similarity: 0.7895)\n",
      "3. ដើម្បីឱ្យអាពាហ៍ពិពាហ៍ប្រព្រឹត្តទៅតាមផ្លូវច្បាប់គូស្រករទាំងពីរត្រូវទៅចុះកិច្ចសន្យាអាពាហ៍ពិពាហ៍នៅមុខមន្ត្រីអត្រានុកូលដ្ឋាននៅគេដ្ឋានខាងស្រី ។ (Similarity: 0.7895)\n",
      "\n",
      "Question: តើចំណែកនៃភាគបម្រុងមានអ្វីខ្លះ ?\n",
      "Top Answers:\n",
      "1. ក្រសួងការបរទេសនិងសហប្រតិបត្តិការអន្តរជាតិស្ថិតនៅក្នុងវិស័យរដ្ឋបាល ។ (Similarity: 0.7895)\n",
      "2. នៅជនបទកម្ពុជាប្រភេទឥន្ធនៈសំខាន់ជាងគេសម្រាប់ចម្អិនអាហារនោះអុស ។ (Similarity: 0.7895)\n",
      "3. ដើម្បីឱ្យអាពាហ៍ពិពាហ៍ប្រព្រឹត្តទៅតាមផ្លូវច្បាប់គូស្រករទាំងពីរត្រូវទៅចុះកិច្ចសន្យាអាពាហ៍ពិពាហ៍នៅមុខមន្ត្រីអត្រានុកូលដ្ឋាននៅគេដ្ឋានខាងស្រី ។ (Similarity: 0.7895)\n",
      "\n",
      "Question: តើដូចម្ដេចដែលហៅថាខ្មាំងរបស់អង្គការ?\n",
      "Top Answers:\n",
      "1. ក្រសួងការបរទេសនិងសហប្រតិបត្តិការអន្តរជាតិស្ថិតនៅក្នុងវិស័យរដ្ឋបាល ។ (Similarity: 0.7895)\n",
      "2. នៅជនបទកម្ពុជាប្រភេទឥន្ធនៈសំខាន់ជាងគេសម្រាប់ចម្អិនអាហារនោះអុស ។ (Similarity: 0.7895)\n",
      "3. ដើម្បីឱ្យអាពាហ៍ពិពាហ៍ប្រព្រឹត្តទៅតាមផ្លូវច្បាប់គូស្រករទាំងពីរត្រូវទៅចុះកិច្ចសន្យាអាពាហ៍ពិពាហ៍នៅមុខមន្ត្រីអត្រានុកូលដ្ឋាននៅគេដ្ឋានខាងស្រី ។ (Similarity: 0.7895)\n",
      "\n",
      "Question: តើប្រតិវេទន៍គយ ត្រូវបំពេញដូចម្តេច ?\n",
      "Top Answers:\n",
      "1. នៅចុងបញ្ចប់សង្រ្គាមលោកលើកទី១ប្រធានាធិបតីសហរដ្ឋអាមេរិកវូដ្រូវិលសុនបានបញ្ជាឲ្យដកកងទ័ពអាមេរិកចេញពីទ្វីបរ៉ុបនេះគឺដើម្បីរួបរួមគ្នាពង្រឹងការអភិវឌ្ឍនិងកសាងសេដ្ឋកិច្ចផ្ទៃក្នុង ។ (Similarity: 0.7870)\n",
      "2. ផ្តល់ជំនួយខាងយោធាប្រឆាំងនិងឥទ្ធិពលសហភាពសូវៀតនៅអាមេរិកខាងត្បូងបង្កើតប្រព័ន្ធសង្គ្រាមផ្កាយប្រឆាំងនិងសូវៀតបើកយុទ្ធនាការប្រឆាំងនិងទាហានគុយបានៅកោះត្រីណាត ។ (Similarity: 0.7870)\n",
      "3. លទ្ធិប្រជាធិបតេយ្យត្រូវបានគេបែងចែកជា៣បែបគឺៈប្រជាធិបតេយ្យបែបប្រធានាធិបតីនិយមប្រជាធិបតេយ្យបែបសភានិយមប្រជាធិបតេយ្យបែបចំរុះ ។ (Similarity: 0.7870)\n",
      "\n",
      "Question: ចូរសិក្សាអំពីធម្មជាតិ នៃសីលតំលៃ ។\n",
      "Top Answers:\n",
      "1. បរិស្ថាននៅទីក្រុងបាងកក ចំនុចចាប់អារម្មណ៍ជាងគេគឺស្ទះចរចរណ៍ ។ (Similarity: 1.0000)\n",
      "2. អាស៊ីអាគ្នេយ៍មានអាកាសធាតុត្រូពិចមូសុង និងអាកាសធាតុអេកា្វទ័រ ។ (Similarity: 1.0000)\n",
      "3. មណ្ឌលម៉ាញេទិច គឺជាដែនម៉ាញេទិចមានឥទ្ធិពលខ្លាំងចំពោះភាគល្អិតនៅក្នុងបរិយាកាសជាន់ខ្ពស់។ (Similarity: 1.0000)\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import json\n",
    "import numpy as np\n",
    "\n",
    "# Step 1: Load the Fine-Tuned Model\n",
    "def load_model(model_path):\n",
    "    \"\"\"\n",
    "    Load the fine-tuned SentenceTransformer model.\n",
    "    \"\"\"\n",
    "    return SentenceTransformer(model_path)\n",
    "\n",
    "# Step 2: Load the Validation Dataset\n",
    "def load_validation_data(json_file):\n",
    "    \"\"\"\n",
    "    Load the validation dataset from the JSON file.\n",
    "    \"\"\"\n",
    "    with open(json_file, \"r\", encoding=\"utf-8\") as file:\n",
    "        data = json.load(file)\n",
    "    \n",
    "    # Extract unique questions and answers for validation\n",
    "    questions = list(set(item[\"sentence1\"] for item in data))\n",
    "    answers = list(set(item[\"sentence2\"] for item in data))\n",
    "    return questions, answers\n",
    "\n",
    "# Step 3: Encode Questions and Answers\n",
    "def encode_texts(model, texts):\n",
    "    \"\"\"\n",
    "    Encode a list of texts using the model.\n",
    "    \"\"\"\n",
    "    return model.encode(texts, convert_to_tensor=True)\n",
    "\n",
    "# Step 4: Perform Semantic Search\n",
    "def semantic_search(question, question_embedding, answers, answer_embeddings, top_k=3):\n",
    "    \"\"\"\n",
    "    Perform semantic search to find the most similar answers for a given question.\n",
    "    \"\"\"\n",
    "    # Compute cosine similarities\n",
    "    similarities = cosine_similarity(\n",
    "        question_embedding.detach().numpy().reshape(1, -1), \n",
    "        answer_embeddings.detach().numpy()\n",
    "    )[0]\n",
    "\n",
    "    # Get top-k answers\n",
    "    top_k_indices = np.argsort(similarities)[-top_k:][::-1]\n",
    "    top_k_answers = [(answers[i], similarities[i]) for i in top_k_indices]\n",
    "    return top_k_answers\n",
    "\n",
    "# Step 5: Validate the Model\n",
    "def validate_model(model, questions, answers, top_k=3):\n",
    "    \"\"\"\n",
    "    Validate the model by encoding and retrieving the most relevant answers for each question.\n",
    "    \"\"\"\n",
    "    print(\"Encoding questions and answers...\")\n",
    "    question_embeddings = encode_texts(model, questions)\n",
    "    answer_embeddings = encode_texts(model, answers)\n",
    "\n",
    "    print(\"\\nPerforming semantic search for validation...\")\n",
    "    for question in questions[:5]:  # Validate on a subset of questions\n",
    "        question_embedding = model.encode(question, convert_to_tensor=True)\n",
    "        results = semantic_search(question, question_embedding, answers, answer_embeddings, top_k=top_k)\n",
    "        \n",
    "        print(f\"\\nQuestion: {question}\")\n",
    "        print(\"Top Answers:\")\n",
    "        for idx, (answer, similarity) in enumerate(results):\n",
    "            print(f\"{idx+1}. {answer} (Similarity: {similarity:.4f})\")\n",
    "\n",
    "# Main Script\n",
    "if __name__ == \"__main__\":\n",
    "    # File paths\n",
    "    model_path = \"fine_tuned_semantic_search_model\"  # Path to fine-tuned model\n",
    "    validation_json_file = \"semantic_search_data.json\"  # JSON file used for validation\n",
    "\n",
    "    # Parameters\n",
    "    top_k = 3  # Number of top answers to retrieve\n",
    "\n",
    "    # Load the fine-tuned model\n",
    "    model = load_model(model_path)\n",
    "    print(\"Model loaded.\")\n",
    "\n",
    "    # Load the validation data\n",
    "    questions, answers = load_validation_data(validation_json_file)\n",
    "    print(f\"Loaded {len(questions)} unique questions and {len(answers)} unique answers.\")\n",
    "\n",
    "    # Validate the model\n",
    "    validate_model(model, questions, answers, top_k=top_k)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unique question validating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded.\n",
      "Loaded 987 unique answers.\n",
      "\n",
      "Encoding the question: តើ​អធិប​តេយ្យ​មាន​ន័យ​ដូចម្ដេច?\n",
      "Encoding answers...\n",
      "\n",
      "Performing semantic search...\n",
      "\n",
      "Question: តើ​អធិប​តេយ្យ​មាន​ន័យ​ដូចម្ដេច?\n",
      "Top Answers:\n",
      "1. ក្រសួងការបរទេសនិងសហប្រតិបត្តិការអន្តរជាតិស្ថិតនៅក្នុងវិស័យរដ្ឋបាល ។ (Similarity: 0.7895)\n",
      "2. នៅជនបទកម្ពុជាប្រភេទឥន្ធនៈសំខាន់ជាងគេសម្រាប់ចម្អិនអាហារនោះអុស ។ (Similarity: 0.7895)\n",
      "3. ដើម្បីឱ្យអាពាហ៍ពិពាហ៍ប្រព្រឹត្តទៅតាមផ្លូវច្បាប់គូស្រករទាំងពីរត្រូវទៅចុះកិច្ចសន្យាអាពាហ៍ពិពាហ៍នៅមុខមន្ត្រីអត្រានុកូលដ្ឋាននៅគេដ្ឋានខាងស្រី ។ (Similarity: 0.7895)\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import json\n",
    "import numpy as np\n",
    "\n",
    "# Step 1: Load the Fine-Tuned Model\n",
    "def load_model(model_path):\n",
    "    \"\"\"\n",
    "    Load the fine-tuned SentenceTransformer model.\n",
    "    \"\"\"\n",
    "    return SentenceTransformer(model_path)\n",
    "\n",
    "# Step 2: Load the Answers Dataset\n",
    "def load_answers(json_file):\n",
    "    \"\"\"\n",
    "    Load the answers from the JSON file.\n",
    "    \"\"\"\n",
    "    with open(json_file, \"r\", encoding=\"utf-8\") as file:\n",
    "        data = json.load(file)\n",
    "    \n",
    "    # Extract unique answers\n",
    "    answers = list(set(item[\"sentence2\"] for item in data))\n",
    "    return answers\n",
    "\n",
    "# Step 3: Encode Texts\n",
    "def encode_texts(model, texts):\n",
    "    \"\"\"\n",
    "    Encode a list of texts using the model.\n",
    "    \"\"\"\n",
    "    return model.encode(texts, convert_to_tensor=True)\n",
    "\n",
    "# Step 4: Perform Semantic Search\n",
    "def semantic_search(question, question_embedding, answers, answer_embeddings, top_k=3):\n",
    "    \"\"\"\n",
    "    Perform semantic search to find the most similar answers for a given question.\n",
    "    \"\"\"\n",
    "    # Compute cosine similarities\n",
    "    similarities = cosine_similarity(\n",
    "        question_embedding.detach().numpy().reshape(1, -1), \n",
    "        answer_embeddings.detach().numpy()\n",
    "    )[0]\n",
    "\n",
    "    # Get top-k answers\n",
    "    top_k_indices = np.argsort(similarities)[-top_k:][::-1]\n",
    "    top_k_answers = [(answers[i], similarities[i]) for i in top_k_indices]\n",
    "    return top_k_answers\n",
    "\n",
    "# Main Script\n",
    "if __name__ == \"__main__\":\n",
    "    # File paths\n",
    "    model_path = \"fine_tuned_semantic_search_model\"  # Path to the fine-tuned model\n",
    "    validation_json_file = \"semantic_search_data.json\"  # JSON file containing answers\n",
    "\n",
    "    # Parameters\n",
    "    top_k = 3  # Number of top answers to retrieve\n",
    "    specific_question = \"ដូចម្តេចដែលហៅថា ការប្រើប្រាស់បច្ចេកទេសទំនើប ?\"  # The question you want to validate\n",
    "\n",
    "    # Load the fine-tuned model\n",
    "    model = load_model(model_path)\n",
    "    print(\"Model loaded.\")\n",
    "\n",
    "    # Load answers\n",
    "    answers = load_answers(validation_json_file)\n",
    "    print(f\"Loaded {len(answers)} unique answers.\")\n",
    "\n",
    "    # Encode the specific question\n",
    "    print(f\"\\nEncoding the question: {specific_question}\")\n",
    "    question_embedding = encode_texts(model, [specific_question])\n",
    "\n",
    "    # Encode all answers\n",
    "    print(\"Encoding answers...\")\n",
    "    answer_embeddings = encode_texts(model, answers)\n",
    "\n",
    "    # Perform semantic search\n",
    "    print(\"\\nPerforming semantic search...\")\n",
    "    results = semantic_search(specific_question, question_embedding, answers, answer_embeddings, top_k=top_k)\n",
    "\n",
    "    # Display results\n",
    "    print(f\"\\nQuestion: {specific_question}\")\n",
    "    print(\"Top Answers:\")\n",
    "    for idx, (answer, similarity) in enumerate(results):\n",
    "        print(f\"{idx+1}. {answer} (Similarity: {similarity:.4f})\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
